% mnras_template.tex
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[a4paper,fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{subfig}
\usepackage{enumitem}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\newcommand{\jbc}[1]{\textcolor{Red}{\bf #1}}
\newcommand{\premise}{\texttt{premise}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Sparse parameter estimation]{Sparse estimation of model-based diffuse thermal dust emission}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[M. O. Irfan et al.]{
Melis O. Irfan,$^{1}$\thanks{E-mail: melis.irfan@cea.fr}
J\'er\^ome Bobin,$^{1}$
%Third Author$^{2,3}$
\\
% List of institutions
$^{1}$Laboratoire CosmoStat, AIM, UMR CEA-CNRS-Paris 7 Irfu, SAp/SEDI, Service d'Astrophysique, \\ CEA Saclay, F-91191 GIF-SUR-YVETTE CEDEX, France.\\
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2017}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
Component separation for the {\it{Planck}} HFI data is primarily concerned with thermal dust emission and the cosmic infrared background (CIB). Current separation methods make use of smoothing, through convolution with a Gaussian beam, to decouple thermal dust emission from CIB anisotropies at small angular scales. In this paper we present a new parameter estimation method, {\texttt{premise}}: Parameter Recovery Exploiting Model Informed Sparse Estimates. This method exploits the sparse nature of thermal dust emission for denoising purposes and makes use of informed, initial parameter estimates to calculate all-sky maps of thermal dust temperature, spectral index and optical depth at 353\,GHz. The data used are simulation data so as to validate {\texttt{premise}}. We find the percentage difference between the {\texttt{premise}} results and the true values to be 2.8, 5.7 and 7.2 per cent at the 1$\sigma$ level across the full sky for thermal dust temperature, spectral index and optical depth at 353\,GHz, respectively. Comparison between {\texttt{premise}} and a GNILC-like method over select regions of the sky reveal {\texttt{premise}} to outperform with increasing success as the signal-to-noise ratio worsens.  
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
Cosmology: diffuse radiation, ISM: dust, extinction, Methods: Data Analysis, Methods: Statistical
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}

Within our Galaxy dust grains, such as carbonaceous and amorphous silicates \citep{draine}, are heated by inter-stellar, UV radiation. The resulting emission, known as thermal dust emission, is the dominant diffuse Galactic emission at frequencies{\mbox{ $>$ 100\,GHz}} \citep{bennett}. Measurements of thermal dust emission reveal the chemistry of the interstellar medium (\citet{comp11}; \cite{jones}), trace interstellar radiation and reveal dust mass and dust column density. Additionally, thermal dust emission is a prominent astrophysical foreground present in measurements of the cosmic microwave background (CMB). In order to infer cosmological information from measurements of the CMB over 100\,GHz, accurate subtraction of thermal dust emission is crucial \citep{planckCompSep}.

Over large angular-scales thermal dust emission can be modelled as blackbody emission, as can the cosmic infrared background (CIB); diffuse infrared emission from dusty galaxies across all redshifts. The CIB is a strong tracer of the star formation history of the Universe \citep{lagache} and its anisotropies shed light on galaxy clustering and dark matter halo distributions \citep{bethermin}. The similarity in spectral shape between thermal dust emission and the CIB adds complexity to their separation. The correlation between thermal dust and $\rm{H_{I}}$ emission can be used to differentiate between thermal dust and the CIB at large angular scales \citep{pr2} but unfortunately this correlation cannot be exploited at scales smaller than several degrees. 

Since the second {\it{Planck}} data release specific attempts to separate thermal dust emission from the CIB over {\it{Planck}} HFI frequencies have been implemented with increasing success. \citet{pr2} fit a modified blackbody (MBB) to the data and present all-sky maps of the three fit parameters: dust temperature, spectral index and optical depth at 353\,GHz. The MBB parameters do not represent the mass-weighted averages of the physical dust properties along the line-of-sight, e.g. the MBB dust temperature is the not the mass-weighted temperature of all the dust particles contributing to the measured emission. Rather, the MBB parameters are simply the empirical parameters which best fit the overall dust emission. This approach is well-suited for component separation, however for an improved understanding of the thermal dust emission mechanism itself more complex models are required (e.g. \citet{draine}; \citet{comp11}; \citet{jones}).

\citet{pr2} introduce a two-stage approach to the customary pixel-by-pixel fit. First they smooth the $N_{\rm{side}}$ 2048 HFI maps to 30 arcmin and fit a MBB with three free parameters to obtain the spectral indices per pixel. Next they fit the 5 arcmin (essentially full resolution) HFI maps, fixing the spectral index values to those obtained at 30 arcmin and allowing two free parameters for the fit. The result being all-sky maps of dust temperature and 353\,GHz optical depth at 5 arcmin and spectral index at 30 arcmin. Smoothing the data averages out the CIB anisotropies to a greater extent than it averages out the spatial correlations of thermal dust emission; the CIB has a flatter angular power spectrum than thermal dust. However the effects of smoothing on thermal dust emission are not negligible; \citet{pr2} note the trade-off between sensitivity to small-scale variations of thermal dust emission and smoothing for component separation purposes. The all-sky maps of thermal dust emission formed from the three fitted parameters display variations of 30 per cent, at all scales, from the Finkbeiner, Davis and Schlegel thermal dust model \citep{fds}. 

\citet{gnilc} present an improvement on the \citet{pr2} method by only smoothing the data within the regions where the CIB anisotropies start to dominate the total signal at small angular scales. Their analysis is performed within the needlet (spherical wavelet) domain, where emissions are modelled as transient waveforms within pixel and harmonic space. \citet{gnilc} account for the `nuisance'  (non-thermal dust) contributions using a covariance matrix formed from noise, CIB and CMB estimates. This nuisance covariance matrix is used to identify regions of the sky where thermal dust emission dominates over CIB, CMB and noise. Within the nuisance dominated regions the data are smoothed and reassessed, if the nuisance term continues to dominate the width of the Gaussian kernel used for smoothing is increased by a factor of two and the process repeated. Therefore the \citet{gnilc} all-sky estimates of thermal dust emission have a range of effective beam sizes from FWHM = 5.0 arcmin, within the high signal-to-noise regions (over 65 per cent of the sky) to FWHM = 21.8 arcmin at high Galactic latitudes. The \citet{gnilc} thermal dust maps are shown to be an improvement on the \citet{pr2} maps when the residual CIB maps from both methods and the reduced $\chi^2$ from the MBB fits are compared. 

\subsection{Limitations and objectives}

Both the \citet{pr2} and \citet{gnilc} method have the disadvantage of being incredibly computationally intensive, as they perform the MBB fit on each pixel for the full resolution {\it{Planck}} data (over fifty million pixels). While the \citet{pr2} method has the ability to provide thermal dust temperature and 353\,GHz optical depth maps at the full 5 arcmin resolution, any variations in the dust spectral index at angular scales smaller than 30 arcmin are smoothed over. Contrarily though, it could be argued that the \citet{pr2} maps are not smoothed enough as strong evidence of CIB contamination still remain. Therefore it is clear that smoothing all pixels within the full sky HFI maps to the same extent is not optimum. The strength of the \citet{gnilc} method is that it locates the areas worst affected by CIB contamination and applies the largest degree of smoothing within these areas. As successful  as this method is it still presents an incomplete picture of the spatial variance of the dust parameters across the sky due to smoothing. We aim to provide a method of parameter estimation for thermal dust emission, contaminated by CIB and noise contributions, which can be applied to all-sky data at full {\it{Planck}} resolution, is robust to a wide range of signal-to-noise ratios across the sky and is faster than a pixel-by-pixel MBB fit. 

We present a new dust parameter estimation method, which we refer to as {\texttt{premise}}: Parameter Recovery Exploiting Model Informed Sparse Estimates. This novel approach builds upon a two-stage procedure:
\begin{itemize}
\item a pre-filtering technique that first performs CIB and noise removal. In the spirit of GNILC, it makes use of the nuisance covariance matrix but further exploits the natural sparsity of the dust emission in the wavelet domain.\\

\item an innovative dust model parameter estimation algorithm that builds upon recent advances in applied mathematics to find the optimum parameter values per pixel using the least squares estimator on the residual between the model maps and the empirical data (thermal dust plus nuisance terms) within the wavelet domain and adding a penalisation factor to favour sparsity.
\end{itemize} 
As {\texttt{premise}} never runs a pixel-by-pixel MBB fit it is faster than the traditional methods and the use of sparsity in the second stage reduces the need for smoothing whilst still providing an improvement in parameter estimation within noise dominated regions. 

Starting with simulation data representing the combination of thermal dust emission, point sources, instrumental noise, CMB and CIB we apply {\texttt{premise}} to produce estimates of the thermal dust MBB parameters. The simulated thermal dust emission was created using a single MBB with three parameters: temperature, spectral index and optical depth at 353\,GHz. Fitting the same model to the {\texttt{premise}} thermal dust emission maps provides a comparison between the fitted and the true parameters which can be used to evaluate {\texttt{premise}}. Additionally, we implement a GNILC methodology \citep{gnilc} to enable comparisons between GNILC and {\texttt{premise}}.

\subsection{Notations}

Throughout this work we make use of numerous notations, we summarise them here for clarity. $x_{\nu_{i}}[k]$ indicates the total flux for pixel $k$ at frequency $\nu_{i}$ and is the combination of CMB, CIB and instrumental noise, point sources and thermal dust emission:
\begin{equation}
x_{\nu_{i}}[k] = x_{\nu_{i}}[k] ^{\rm{CMB}} + x_{\nu_{i}}[k] ^{\rm{CIB}} + x_{\nu_{i}}[k] ^{\rm{noise}} + x_{\nu_{i}}[k] ^{\rm{ps}} + x_{\nu_{i}}[k] ^{\rm{dust}}.
\end{equation}  
When considering all of the observational frequencies simultaneously the vector form of the above equation can be used:
\begin{equation}
X[k] = X[k] ^{\rm{CMB}} + X[k] ^{\rm{CIB}} + X[k] ^{\rm{noise}}  + X[k] ^{\rm{ps}} + X[k] ^{\rm{dust}}
\end{equation}  
and for all frequencies and all pixels we have the matrix form:
\begin{eqnarray}
{\bf{X}} = {\bf{X}} ^{\rm{CMB}} + {\bf{X}} ^{\rm{CIB}} + {\bf{X}} ^{\rm{noise}} + {\bf{X}} ^{\rm{ps}} + {\bf{X}}^{\rm{dust}}.
\end{eqnarray} 
The forward wavelet transformation of single frequency map is denoted as  $x_{\nu_{i}} {\bf{\Phi}}$, with $0 < j < J$ being the number of the wavelet scale. At each scale the transformations produces $2^{j-1}$ coefficients which consist of the coarse scale ($c$) and wavelet coefficients ($w$):
\begin{eqnarray}
x_{\nu_{i}} \Phi  = \alpha_{\nu_{i}} = [c_{\nu_{i}}, w_{\nu_{i}}^{(j=1)} ... w_{\nu_{i}}^{(j=J)}].
\end{eqnarray} 
The paper is organised as follows: section \ref{sec:data} introduces the simulation data alongside the ancillary data used and the regions of the sky investigated, section \ref{sec:method} details the steps within {\texttt{premise}} and section \ref{sec:results} presents our results in comparison with results obtained via the GNILC methodology.  

\section{Data and preprocessing}
\label{sec:data}
\subsection{Simulated Thermal Dust}
The data used in this paper are simulation data, formed from equation~(\ref{eq:bbinten}).
The GNILC \footnotemark  all-sky temperature and spectral index maps provide the required temperature ($T$)
 and spectral index ($\beta$), the {\it{Planck}} FFP8 \citep{ffp} thermal dust map at 353\,GHz provides the normalisation factor and B denotes the blackbody function.  
\footnotetext{http://pla.esac.esa.int/pla/}
\begin{eqnarray}
x_{\nu_{i}} ^{\rm{dust}} = x_{353\, \rm{GHz}}^{\rm{ffp8}} \times \frac{B(T,  \nu)}{B(T, 353 \, \rm{GHz})} \times
    \left(\frac{\nu}{353 \, \rm{GHz}}\right)^{\beta} \times \rm{cc}^{-1}.
\label{eq:bbinten}
\end{eqnarray}
The single spectral index model was preferred over the two-index model \citep{meis} as the two-index model has been shown to improve the fit to the data when frequencies below 353\,GHz are used. Equation~(\ref{eq:bbinten}) was evaluated at frequencies 353, 545, 857 and 3000\,GHz to emulate {\it{Planck}} HFI data used in combination with the 100\,$\mu$m IRIS map \citep{iris}. These fluxes were multiplied by their inverse colour corrections ($\rm{cc}^{-1}$) to further align them with real data. The colour corrections were calculated across the {\it{Planck}} HFI bandpasses for frequencies 353, 545 and 857\,GHz. The 3000\,GHz colour correction factors were taken from Table 3 of \citet{iris}. 

Simulation data were not created for the lowest two HFI frequencies (100 and 217\,GHz) following the method of \citep{gnilc}, which fits a MBB model to the GNILC 353, 545, 857 and 3000\,GHz maps. The 353\,GHz FFP8 data were neither smoothed nor downgraded, so all our simulation data are at $\rm{N_{side}}$ 2048 with a FWHM of $\sim$ 5 arcmin. 

\subsection{Regions}

We compute {\texttt{premise}} on the full sky, however as GNILC is a computationally intensive algorithm we only run the GNILC methodology on four test regions and conduct our comparison within these four, 256X256 pixel regions. The regions comprise of a high signal-to-noise region in the Galactic plane (region 1), a low signal-to-noise region at intermediate latitude (region 2), a fair signal-to-noise region at intermediate latitude (region 3) and a low signal-to-noise polar region (region 4). The location of these regions within a {\texttt{HEALPix}} sphere is shown in Fig.~\ref{fig:globe}. 

\begin{figure}
	\centering
	\includegraphics[width=0.99\linewidth]{patchesOnGlobe}
	\caption{The location on the sphere of the four 256X256 pixel regions chosen for analysis.}
	\label{fig:globe}
\end{figure}

\subsection{Simulated CIB, CMB, noise and point sources}

The simulated total flux maps used differ for the GNILC methodology and {\texttt{premise}}. While GNILC includes the CMB within its covariance matrix of nuisance terms, {\texttt{premise}} is intended for use on CMB subtracted maps. In \cite{lgmca}, we introduced a sparsity-based component separation coined L-GMCA to estimate a precise CMB map from the Planck data. More specifically, we showed that the estimated CMB map has very low foreground contamination. Consequently, the L-GMCA CMB map will be removed prior to applying the \premise \, algorithm. The total flux maps used for the GNILC methodology were constructed as: 
\begin{eqnarray}
{\bf{X}}^{\rm{GNILC}} =  {\bf{X}}^{\rm{dust}} +  {\bf{X}}^{\rm{CIB}} +  {\bf{X}}^{\rm{CMB}}  +   {\bf{X}}^{\rm{noise}}  + {\bf{X}}^{\rm{ps}},
\label{eq:gnilcTotS}
\end{eqnarray}
where the FFP8 simulations provide the instrumental noise, point sources and CMB contributions. A 3000\,GHz CIB map of $\rm{N_{side}}$ 2048 and FWHM 5 arcmin was created using the methodology detailed in Appendix C of \citet{pr2}. Gaussian noise with a median level of 0.06 MJy $\rm{sr}^{-1}$ \citep{pr2} was used for the 3000\,GHz noise map. No CMB nor point source contributions were included at 3000\,GHz.
The total flux maps used for {\texttt{premise}} were constructed as: 
\begin{eqnarray}
{\bf{X}}^{\rm{{\texttt{premise}}}} = {\bf{X}}^{\rm{dust}} + {\bf{X}}^{\rm{\widetilde{CIB}}} +  {\bf{X}}^{\tilde{n}}(\nu)  + {\bf{X}}^{\rm{ps}},
\label{eq:waveTotS}
\end{eqnarray}
where ${\bf{X}}^{\rm{\widetilde{CIB}}}$  and ${\bf{X}}^{\tilde{n}}(\nu)$ are simulated CIB and instrumental noise maps featuring fractional CMB emission, representative of the CIB and noise properties of an intensity map after the L-GMCA CMB subtraction.

\subsection{LAB data and the large-scale CIB contribution}

The CIB is seen to contribute to the overall measured intensity in two ways: small-scale variations and a large-scale intensity which manifest as a constant, additive offset to the pure thermal dust intensity. Fig.~\ref{fig:ciboffset} shows a 1D slice of region 3 at 353\,GHz. The green line shows the total emission while the blue line shows the pure thermal dust emission. The large-scale CIB contribution can clearly be seen as a positive offset while the small-scale CIB contributions, alongside instrumental noise, are seen as Gaussian-like variations around the thermal dust mean level. 

Before the simulated maps of total emission can be processed the large-scale CIB offsets must first be subtracted. The removal of the large-scale CIB offset is achieved through the use of LAB $\rm{H_{I}}$ Survey data. These data are available in the form of all-sky {\texttt{HEALPix}} maps \citep{healpix} at $\rm{N_{side}}$ 512 and a FWHM of \mbox{$\sim$ 36 arcmin}. \footnotemark \footnotetext{https://lambda.gsfc.nasa.gov} The constant CIB offsets for each frequency were calculated using the method described in \citet{pr2}, see Appendix \ref{sec:apA} for details. The resulting offsets were found to be 0.126, 0.331, 0.641 and 0.657 MJy $\rm{sr^{-1}}$ for 353, 545, 857 and 3000\,GHz respectively.  These values were subtracted from the total flux maps. The mean values for the simulated CIB maps are 0.140, 0.366, 0.718 and 0.718 MJy\,sr$^{-1}$, placing an 8.5 -- 11 per cent error on the linear regression method.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{ciboffset353face6}
\caption{One dimensional slice of region 3 at 353\,GHz. The green line represents thermal dust plus CIB plus instrumental noise, the blue line is just thermal dust.}
\label{fig:ciboffset}
\end{figure}

\subsection{GNILC and {\texttt{premise}} preprocessing}

We implement two algorithms: our own, {\texttt{premise}} and a GNILC-like methodology. The principal steps for both algorithms are summarised in Fig.~\ref{fig:methFlo}: removing large-scale CIB offsets, filtering out noise and small-scale CIB anisotropies and fitting a MBB. GNILC makes use of a covariance matrix of the combined nuisance terms to locate dimensions within signal subspace where the desired signal, thermal dust emission in this case, is dominant. The nuisance terms are the small-scale CIB contributions, the CMB and instrumental noise. As this paper is concerned with simulation data the exact CIB, CMB and instrumental noise maps used to make the total flux maps are given as nuisance term estimates, therefore the estimation of the nuisance covariance matrix is unrealistically perfect. The GNILC methodology for filtering total flux maps to produce pure thermal dust estimates that we implement is not identical to that described in \citet{gnilc}, therefore we detail our GNILC filtering technique in Appendix \ref{sec:apB}, highlighting any differences. 

\begin{figure}
\centering
\includegraphics[width=0.99\linewidth]{methodDiagram}
\caption{The principal steps of the both the GNILC and {\texttt{premise}} algorithms.}
\label{fig:methFlo}
\end{figure}

After the initial filtering the following MBB model is fit pixel-by-pixel to the GNILC estimate of pure thermal dust emission: 
\begin{eqnarray}
x_{\nu_{i}} = \tau_{353} \times B(T,  \nu) \times \left(\frac{\nu}{353 \, \rm{GHz}}\right)^{\beta} \times \rm{cc}^{-1},
\end{eqnarray}
where the optical depth at 353\,GHz ($\tau_{353}$), the dust temperature ($T$) and the thermal dust spectral index ($\beta$) are the free parameters in the fit. The inverse colour correction ($\rm{cc}^{-1}$) also depends on $T$ and $\beta$.

From Fig.~\ref{fig:methFlo} it can be seen that both GNILC and {\texttt{premise}} filter the total flux maps to produce thermal dust estimates. For GNILC, the principal aim is to construct all-sky thermal dust estimates which can then provide values for the MBB parameters through a pixel-by-pixel fit. The goal of {\texttt{premise}} is the inverse; the {\texttt{premise}} filtered dust estimates are only of use as inputs to the MBB fit, our aim is to calculate the thermal dust MBB parameters as accurately as possible. 

\subsection{Dealing with point sources} \label{scontrib}

Point sources were removed from the 353, 545 and 857\,GHz data using the appropriate FFP8 masks for each frequency. The {\texttt{premise}} algorithm is capable of processing masked data as the use of the discrete wavelet transform allows for the reconstruction of the full wavelet, providing the Shannon sampling condition is fulfilled. The GNILC methodology, however, requires each data pixel to provide astrophysical information. Therefore the masked GNILC data were inpainted using a morphological component analysis technique \citep{inpaint} applied to each frequency map independently.   

\section{The\, \premise \,method}
\label{sec:method}

Estimating the thermal dust parameters per pixel from the Planck data raises two important issues: i) it requires solving large-scale ({\it i.e.} $50.10^6$ pixels for full-resolution Planck data) non-linear parameter estimation problems and ii) it is highly sensitive to nuisance terms such as noise, CIB and CMB, which explains why standard pixel-based fitting performs poorly in noise-dominating region of the sky. The method of filtering nuisance contributions prior to applying pixel-based fitting has two limitations: i) it only uses statistical information about the nuisance term but does not account for the statistics of the dust to be retrieved, and ii) pixel-based fitting will still be sensitive to dust estimation biases induced by the dust filtering procedure. For that purpose, the proposed\, \premise \,method builds upon a two-stage approach:
\begin{itemize}
	\item A nuisance pre-filtering procedure that performs CIB plus noise removal to provide a clean dust emission estimate. Unlike GNILC, it further accounts for the natural sparsity of the dust emission in the wavelet domain, which allows for a more accurate estimation of the dust emission with higher spatial resolution.
	\item A new parameter estimation procedure that minimizes a non-linear least-square cost function to which an extra penalization is added to favour sparse parameter maps in the wavelet domain. This procedure allows for a fast and effective full-sky estimation and accounts for the naturally sparse distribution of the temperature and spectral index maps in the wavelet domain.
\end{itemize}

\citet{pr2} split their MBB fit into two stages to combat parameter degeneracies introduced by the CIB anisotropies. We too experienced such degeneracies and so also opted for the two stage approach, choosing to first calculate $T$ and $\beta$ and then, holding these first two parameters fixed, we calculate $\tau_{353}$.

\subsection{Pre-filtering step} \label{cibcontrib}

\jbc{I will put the description of the code here in the next round.}


\subsection{Fitting $T$ and $\beta$ from the MBB model}
Fitting for the spectral index and the temperature in the MBB model requires dealing with the non-linear relationship between these parameters and the observed pixel. For a given pixel $k$, this reads as:
\begin{eqnarray}
\min_{\beta_k,T_k}  \sum_{i} \left({x}_{\nu_i}[k] - { x}_{\nu_i}[k]^{\mbox{dust}}(\beta_k,T_k) \right)^2
\label{eq:pixfit}
\end{eqnarray}
In\, \premise \,and unlike currently available pixel-based fitting methods, we propose estimating full-sky maps of the parameters while accounting for their strong correlation across pixels. This further requires solving a minimization problem of the form:
\begin{equation}
\min_{\beta,T} \lambda_b \| \beta {\bf \Phi}\|_{\ell_1} + \lambda_t \| T {\bf \Phi}\|_{\ell_1} + \sum_{i,k} \left({x}_{\nu_i}[k] - \mathcal{M}_{\nu_i}[k] { x}_{\nu_i}[k]^{\mbox{dust}}(\beta_k,T_k) \right)^2
\end{equation}
\begin{equation}
\min_{\beta,T} \lambda_b \| \beta {\bf \Phi}\|_{\ell_1} + \lambda_t \| T {\bf \Phi}\|_{\ell_1} + \sum_{i,k} \left({x}_{\nu_i}[k] - \mathcal{M}_{\nu_i}[k] { x}_{\nu_i}[k]^{\mbox{dust}}(\beta_k,T_k) \right)^2
\label{eq:cost_function}
\end{equation}
where the quadratic term measures the discrepancy between the observations and the MBB model. The penalization terms enforce the sparsity of the estimated $T$ and $\beta$ in the wavelet domain. More precisely, the penalization terms are described as $\| \beta {\bf \Phi}\|_{\ell_1} = \sum_j | c_j|$, where  $c_j = [\beta {\bf \Phi}]_j$ are the wavelet coefficients of the $\beta$ maps. In the next section, we will make of the undecimated and isotropic wavelet transform \citep{starck2007undecimated}. The term $\mathcal{M}_{\nu_i}[k]$ stands for the point source mask for frequency $\nu_i$ and pixel $k$. Masks take the value of $1$ when no source is present and $0$ otherwise.

We will show section~\ref{parref} how such a penalized non-linear least-square minimization \textcolor{red}{we also need to have the UK vs US spelling argument} problem can be solved using an iterative thresholding algorithm. However, since the MBB model is non-linear, these algorithms can be sensitive to the initial point from which the algorithm starts. To alleviate this issue, we propose a two-stage fitting approach that is composed of: i) a fast initialization procedure that builds upon a quadtree decomposition of the data in the wavelet domain, and ii) a refinement stage based on iterative thresholding algorithm.

\subsubsection{Fast wavelet-based initialization}

For {\texttt{premise}} the fit to a MBB is more complex than a standard pixel-by-pixel fit; it makes use of the quadtree technique. A quadtree recursively divides the given data into quarters until a particular criteria is no longer achieved within the data patch. The quadtree used in this method recursively divides a square of data until the number of data points (in this case, reduced $\chi^{2}$ values) with a value greater than 2 within the patch are less than 10 per cent of the total number of data points. A lower limit of 8 is enforced to ensure that the quadtree cannot split the data into patches smaller than 8X8 pixels. The quadtree is also prohibited from splitting a patch into quarters if one or more of those quarters contains only masked data. Fig.~\ref{fig:patches} shows the patches selected by the quadtree over the full sky. The patches sizes are visibly smaller within and near to the Galactic plane where the total flux across neighbouring pixels is less consistent than at high latitudes. Very close to the Galactic centre several patch sizes larger than their neighbouring patches can be seen: these areas contain so many masked pixels (due to point sources) that the quadtree is forbidden from dividing the region into smaller areas. 

\begin{figure}
\centering
\subfloat{\includegraphics[width=0.99\linewidth]{fullskyPatches}}
\caption{All-sky map of the different patches chosen by the quadtree.}
\label{fig:patches}
\end{figure}

The {\texttt{premise}} fitting process works on each of the twelve 2048X2048, {\texttt{HEALPix}} 2D faces which make up the full sky as follows:
\begin{enumerate}[label*=\arabic*.]
\item The noise covariance matrix ($R_{n}$) of the full data face is calculated within the wavelet domain for four wavelet scales:
\begin{eqnarray}
R_{n}^{j} = \frac{1}{2048^2} \left( {\bf{ \Phi}}^{j} X_{n} \;  (\Phi^{j} X_{n})^{T} \right),
\end{eqnarray} 
where $\Phi^{j} X_{n}$ is the wavelet transform of the instrumental noise plus the CIB anisotropies (large-scale CIB offsets removed) and $^{T}$ denotes the matrix transpose.  
\item The region is split into patches of 128X128
\item Each patch is treated as a superpixel: the mean flux density for each frequency represents the whole patch at that frequency	
\item A MBB fit to each superpixel yields the parameters required to form a thermal dust estimate for each frequency at that superpixel ($P$).
\item The data-model residual is calculated per original pixel:
$$
\rm{Residual}[k] = \rm{Total \, Flux}[k] -  \rm{Estimated \, thermal \, dust \, flux}[k] 
$$
\item This residual matrix (${\rm{N_{obs}}}$ by ${\rm{N_{pix}}}$ ) is transformed into the wavelet domain using four wavelet scales ($\Phi^{j} X_{r}$). 
\item The reduced $\chi^{2}$ at each of the four wavelet scales is calculated for each pixel:
\begin{eqnarray}
\chi^{2}_{\rm{red}}[k] = \Phi^{j} X_{r}  (\Phi^{j} X_{r})^{-1} \, \Phi^{j} X_{r} 
\end{eqnarray}
As there is only one degree of freedom the reduced $\chi^{2}$ is just the $\chi^{2}$.
\item The reduced $\chi^{2}$ is fed back into the quadtree to split the data into patches where $T$ and $\beta$ are constant enough to be well characterised by a MBB with a single value of $T$ and $\beta$ across all pixels within the patch. The higher wavelet scales are used for larger patch sizes so when deciding whether or not to split a 64X64 patch the quadtree uses the reduced $\chi^{2}$ values from the fourth wavelet scale, for a 16X16 patch the first wavelet scale is used. The final patches chosen are the quadtree determined superpixels.  
\item The fit is re-run fit on the quadtree chosen superpixels and initial maps of $T$ and $\beta$ estimates are obtained.
\end{enumerate}

Running the MBB fit on super-pixels, as opposed to the true pixels, reduces the computational time by, at minimum, a factor of 64 (the minimum patch area being 64 pixels).  

\subsubsection{Parameter refinement} \label{parref}
\jbc{I'll re-write this part in the next round}
The above quadtree-based fitting technique provides estimates of the MBB parameters that are quick to compute and robust to noise contamination. However, since it is based on superpixel fitting, it provides a rough low resolution estimate of those parameters. In this section, we propose recasting the estimation of $\beta$ and $T$ as a non-linear inverse problem. For that purpose, one can rewrite the observations $\bf X$ as a function of the sought-after parameters for each frequency $\nu$ and pixel $k$ as:
\begin{eqnarray}
{\bf X}_{\nu,k} = {\bf X}_{\nu,k}^{\mbox{dust}}(\beta_k,T_k) + {\bf R}_{\nu,k},
\end{eqnarray}
where ${\bf X}_{\nu,k}^{\mbox{dust}}(\beta_k,T_k)$ is the dust contribution and follows the MBB model described in Equation~\ref{eq:bbinten}. The term ${\bf R}_{\nu,k}$ stands for residuals of CIB and instrumental noise after pre-filtering. The standard fitting approach, which is implemented in GNILC, consists in computing at each pixel $k$ the least-square estimates $\beta_k$ and $T_k$ that minimize the Euclidean distance between the data ${\bf X}_{\nu,k}$ and the model ${\bf X}_{\nu,k}^{\mbox{dust}}(\beta_k,T_k)$. However, it is well-known that the least-square estimate is generally highly sensitive to the presence of noise or spurious contamination. Consequently, it is customary in statistics to resort to penalized least-square estimates. This penalisation provides robustness to noise by favouring certain solutions with desired properties. In this framework, we propose to profit from the natural sparsity of the $\beta$ and $T$ maps in the wavelet domain so as to build new estimates of $\beta$ and $T$ \citep{starckBook}.

Since the $\ell_1$ norm is not differentiable, minimizing the cost function (Eq.~\ref{eq:cost_function}) cannot be achieved using standard gradient descent methods. The recently introduced proximal algorithms \citep{parikh2014proximal} can provide the perfect framework to design an effective minimizer for the problem in Eq.~\ref{eq:cost_function}. Specifically, we will make use of the FISTA algorithm, which is an iterative projected gradient descent algorithm introduced in \citet{beck2009fast}. While the FISTA algorithm algorithm has been specifically designed to solve penalized least-square problems with linear models, projected gradient descent techniques have also shown to be well-suited for tackling similar problems with non-linear problems \cite{teschke09}, which makes it a perfect fit to solve Eq.~\ref{eq:cost_function}.

The parameter optimisation step of {\texttt{premise}} builds upon an iterative estimation procedure, composing of the following two stages:
\begin{itemize}
\item{\it Gradient descent: } this steps consists in performing a single gradient descent step of the data fidelity term with respect to $\beta$ and $T$:
\begin{eqnarray}
\beta_k^{(1/2)}  =  \beta_k^{(t)} + \alpha g_{\beta,k}^T {\bf \mathcal{M}}_{\nu_i}[k]  \left({\bf X}_k - {\bf \mathcal{M}}_{\nu_i}[k] {\bf X}_{k}^{\mbox{dust}}(\beta_k^{(t)},T_k^{(t)})  \right),
\end{eqnarray}
\begin{eqnarray}
T_k^{(1/2)} =  T_k^{(t)} + \alpha g_{T,k}^T {\bf \mathcal{M}}_{\nu_i}[k] \left({\bf X}_k - {\bf \mathcal{M}}_{\nu_i}[k]{\bf X}_{k}^{\mbox{dust}}(\beta_k^{(t)},T_k^{(t)})  \right).
\end{eqnarray}
where  $\alpha_t$ and $\alpha_b$ are the gradient path length. The term $g_{\beta,k}$ is a vector whose $i$-th is equal to $\frac{\partial  {\bf X}_{\nu_i,k}^{\mbox{dust}}}{\partial \beta_k}(\beta_k^{(t)},T_k^{(t)})  $. Similarly, $g_{T,k}$ is a vector whose $i$-th is equal to $\frac{\partial  {\bf X}_{\nu_i,k}^{\mbox{dust}}}{\partial T_k}(\beta_k^{(t)},T_k^{(t)})$. Simply put, the vector $g_{\beta,k}$ ({\it resp.} $g_{T,k}$) is the derivative of the MBB model at pixel $k$, ${\bf X}_{k}^{\mbox{dust}}(\beta,T)$, with respect to $\beta$ ({\it resp.} $g_{T,k}$). These quantities are computed analytically from the expression of the MBB model.\\

\item{\it Projection step: } In this step, the estimated $\beta$ and $T$ maps are thresholded in the wavelet domain, which is formalizes as:
\begin{eqnarray}
\beta_k^{(t + 1)} & = & \mathcal{S}_{\alpha \lambda_{\beta}}\left( \beta^{(1/2)} {\bf \Phi} \right) {\bf \Phi}^{-1}, \\
T_k^{(t + 1)} & = & \mathcal{S}_{\alpha \lambda_{T}}\left( T^{(1/2)} {\bf \Phi} \right) {\bf \Phi}^{-1}.
\end{eqnarray}
where $\mathcal{S}_{\tau}$ stands for the soft-thresholding operator with threshold $\tau$, which is described as follows for some scale $u$:
\begin{equation}
\mathcal{S}_{\tau}(u) = \left \{
\begin{array}{ll}
u - \tau \mbox{sign}(u) & \mbox{ if } |u| > \tau \\
0 & \mbox{ otherwise}
\end{array}
\right.
\end{equation}
\begin{center}
	\centering
	\vspace{0.25in}
	\begin{tabular}{|c|} \hline
		\begin{minipage}[hbt]{0.95\linewidth}
			\vspace{0.15in}
			
			\textsf{{\bf Initialize} $\beta^{(0)}$ and $T^{(0)}$ with the quad-tree based fitting technique,}\\
			
			\textsf{\bf At each iteration $t$.}\\
			
			\hspace{0.2in} \textsf{1 - Gradient descent step for each pixels $k$} \\ \\
				\hspace{0.4in}  $\beta_k^{(1/2)}  =  \beta_k^{(t)} + \alpha g_{\beta,k}^T {\bf \mathcal{M}}_{\nu_i}[k]\left({\bf X}_k - {\bf \mathcal{M}}_{\nu_i}[k]{\bf X}_{k}^{\mbox{dust}}(\beta_k^{(t)},T_k^{(t)})  \right) $\\
				\hspace{0.4in}  $T_k^{(1/2)}  =  T_k^{(t)} + \alpha g_{T,k}^T {\bf \mathcal{M}}_{\nu_i}[k] \left({\bf X}_k - {\bf \mathcal{M}}_{\nu_i}[k]{\bf X}_{k}^{\mbox{dust}}(\beta_k^{(t)},T_k^{(t)})  \right) $\\
			
			\hspace{0.2in} \textsf{2 - Thresholding step} \\ \\
				\hspace{0.4in}  $\beta_k^{(t + 1)}  =  \mathcal{S}_{\alpha \lambda_{\beta}}\left( \beta^{(1/2)} {\bf \Phi} \right) {\bf \Phi}^{-1} $\\
				\hspace{0.4in}  $T_k^{(t + 1)}  =  \mathcal{S}_{\alpha \lambda_{T}}\left( T^{(1/2)} {\bf \Phi} \right) {\bf \Phi}^{-1}$ \\

			\textsf{\textbf{Stop} when a given criterion is valid.}
			
			\vspace{0.15in}
		\end{minipage}
		\\\hline
	\end{tabular}
	\vspace{0.25in}
\end{center}

\paragraph{Tuning the parameters}
In this paragraph, we discuss how the different parameters of the parameter refinement stage are tuned.
\begin{itemize}
\item{Gradient path length:} The algorithm converges to a stationary point of the problem of Eq. \ref{eq:cost_function} provided $\alpha \leq  \min_k 1/||H_k||_2$, where $H_k$ is the Hessian matrix of the data fidelity term with respect to $\beta$ and $T$. The spectral norm $\| H_k\|_2$ is the largest singular value of the Hessian matrix. The proposed iterative thresholded gradient descent technique builds upon a local linear approximation of the data fidelity term about the current estimate at iteration $t$. The same linear approximation yields $\alpha \leq  \min_k 1/(g_{\beta,k}^T g_{\beta,k} +g_{T,k}^T g_{T,k})$, which is computed analytically at each iteration.\\

\item{Thresholds:} A careful choice of the thresholds $\lambda_b$ and $\lambda_t$ are essential since they allow to tune the trade off between the data fidelity term and the sparse penalization. For that purpose, we make use of a very effective, heuristic approach to choose these parameters automatically. The goal of the thresholding stage is to essentially save the wavelet coefficients with significant amplitude while removing noise or non-sparse elements. In this context, the standard deviation of the non-sparse elements can be computed using the MAD (Median Absolute Deviation) \citep{starckBook}; the thresholds are then chosen as $\lambda_b = 2\, \sigma_{\mbox{MAD}(\beta^{(1/2)})}$ and $\lambda_T = 2 \, \sigma_{\mbox{MAD}(T^{(1/2)})}$. In practice, the thresholds are computed independently at each of the five wavelet scales scales. \\

\item{Stopping criterion:}	The proposed iterative algorithm stops when the relative variation of the estimates between two consecutive iterations is lower than a given level: 
$$
 \max_k \max_{\beta_k,T_k} \left(  \frac{\|\beta_k^{(t+1)} - \beta_k^{(t)} \|_{\ell_2}}{\|\beta_k^{(t)} \|_{\ell_2}},   \frac{\|T_k^{(t+1)} - T_k^{(t)} \|_{\ell_2}}{\|T_k^{(t)} \|_{\ell_2}}   \right) \leq 10^{-6}
$$
\end{itemize}

\end{itemize}

\subsubsection{Optical depth at 353\,GHz}

Up to this point we have only dealt with two of the three MBB parameters, temperature and spectral index. Both the fast wavelet-based initialisation and the parameter refinement steps can be utilised to produce optical depth values just as they do for the temperature and spectral index. However, due to the degeneracies between the three parameters introduced through residual CIB and noise, {\texttt{premise}} produces less accurate optical depth estimates than desired. Both \citet{pr2} and \citet{gnilc} encounter these degeneracies and deal with them through smoothing; in \citet{pr2} the spectral index maps are smoothed to 30 arcmin while in \citet{gnilc} all three parameters are smoothed within the lowest signal to noise regions of the sky. 

As one of our primary goals is to maintain the full resolution of the HFI data {\texttt{premise}} is specialised to trace the spatial variations of the dust temperature and spectral index, to the detriment of the overall normalisation factor (the optical depth). At this point it is therefore useful to introduce a thermal dust template to recover the normalisation factor. The total flux 857\,GHz map is known to suffer only minimally from CIB contamination and so provides us with the opportunity to recover the 353\,GHz optical depth at 5 arcmin using the {\texttt{premise}} estimates for temperature and spectral index: 
\begin{eqnarray}
\tau_{353} = \frac{x_{857} \times \rm{cc}}{B(T,  857\,\rm{GHz}) \times \left(\frac{\nu}{353 \, \rm{GHz}}\right)^{\beta} }.
\end{eqnarray}

As the total flux data include point sources, a masked and inpainted \citep{inpaint} version of the total flux map is used to recovery the optical depth at 353\,GHz. 

\section{Results}
\label{sec:results}

\subsection{Comparison of parameter estimation within Regions 1--4}

In the following section we compare the GNILC and {\texttt{premise}} fitted $\tau_{353}$, $T$ and $\beta$ values for Regions 1--4 to the true $\tau_{353}$, $T$ and $\beta$. Fig.~\ref{fig:histAll} shows histograms of the actual differences between the true parameter values and those derived from the GNILC methodology/{\texttt{premise}}. Fig.~\ref{fig:mapsAll} shows maps of the percentage differences between the true parameter values and those derived from the GNILC methodology/{\texttt{premise}}. For the region 1, the region with the largest thermal dust signal to noise ratio, the GNILC $\tau_{353}$ and $\beta$ parameters show smaller differences with the true parameter values than {\texttt{premise}}. The opposite being true for the temperature estimates. When the signal to noise ratio is high {\texttt{premise}} and the GNILC methodology perform comparably but overall GNILC provides the best parameter estimates. However for the other regions, outside of the Galactic plane smaller differences between the true and fitted parameters can be seen for {\texttt{premise}}, specifically regions 2 and 4, were the thermal dust signal-to-noise ratio is lowest. The advantage of {\texttt{premise}} is clear when the diverse range of signal to noise across the sky is considered. 

\begin{figure}
\centering
\subfloat{\includegraphics[width=0.85\linewidth]{histFace1}}\,
\subfloat{\includegraphics[width=0.85\linewidth]{histFace5}}\,
\subfloat{\includegraphics[width=0.85\linewidth]{histFace6}}\,
\subfloat{\includegraphics[width=0.85\linewidth]{histFace9}}\,
\caption{Histograms of the differences between model and true temperature/spectral index for the GNILC methodology and {\texttt{premise}}.  Results for region 1 to 4 shown from top to bottom.}
\label{fig:histAll}
\end{figure}

\begin{figure*}
\centering
\subfloat[][]{\includegraphics[width=0.5\linewidth]{mapsFace1}\label{a}}
\subfloat[][]{\includegraphics[width=0.5\linewidth]{mapsFace5}\label{b}}\,
\subfloat[][]{\includegraphics[width=0.5\linewidth]{mapsFace6}\label{c}}
\subfloat[][]{\includegraphics[width=0.5\linewidth]{mapsFace9}\label{d}}
\caption{Maps of percentage differences between model and true temperature/spectral index for the GNILC methodology and {\texttt{premise}} for each pixel.Results for region 1 \protect\subref{a},  2 \protect\subref{b}, 3 \protect\subref{c} and 4 \protect\subref{d}}
\label{fig:mapsAll}
\end{figure*}

\subsection{Full sky {\texttt{premise}}}

The all-sky {\texttt{premise}} thermal dust temperature, spectral index and optical depth at 353\,GHz are shown in Fig.~\ref{fig:dustmaps}. Table~\ref{tab:accstats} states the percentage difference between true and {\texttt{premise}} estimated parameter values for 50 per cent (medium), 68.3 per cent (1$\sigma$), 95.4 per cent (2$\sigma$) and 99.7 per cent (3$\sigma$) of the full sky.

\begin{table}
 \caption{The percentage difference between true and {\texttt{premise}} estimated thermal dust parameters for various percentiles.}
 \label{tab:accstats}
 \begin{tabular}{l| c| c| c| c}
\hline
{\bf{Value}} &{\bf{ Medium $\% \Delta$}}  & {\bf{1$ \sigma \, \% \Delta$}}  & {\bf{2$ \sigma \, \% \Delta$}}  & {\bf{3$ \sigma \, \% \Delta$}}  \\ 
Temperature & 1.7 & 2.8 & 8.0 & 16.5 \\
Spectral index & 3.4 & 5.7 & 15.4 & 25.6 \\
Optical depth (353\,GHz) & 3.7 & 7.2 & 31.2 & 77\\
%353\,GHz flux & 5.3 & 10.7 & 44.8 & 116.3 \\
%545\,GHz flux & 3.6 & 7.4 & 32.4 & 85.7 \\
%857\,GHz flux & 2.9 & 5.7 & 25.6 & 69.3 \\
%3000\,GHz flux & 3.4 & 6.0 & 24.8 & 78.9 \\
  \hline
 \end{tabular}
\end{table}

\begin{figure*}
\centering
%\subfloat{\includegraphics[width=0.5\linewidth]{fullsky353}}
%\subfloat{\includegraphics[width=0.5\linewidth]{fullsky545}}\,
%\subfloat{\includegraphics[width=0.5\linewidth]{fullsky857}}
%\subfloat{\includegraphics[width=0.5\linewidth]{fullsky3000}}\,
\subfloat{\includegraphics[width=0.65\linewidth]{fullskyTemp}}\,
\subfloat{\includegraphics[width=0.65\linewidth]{fullskyBeta}}\,
\subfloat{\includegraphics[width=0.65\linewidth]{fullskyTau}}\,
%\caption{All-sky maps of thermal dust emission produced by {\texttt{premise}} filtering at 353 ({\it{top left}}), 545 ({\it{top right}}), 857 ({\it{middle left}}) and 3000\,GHz ({\it{middle right}}). These maps have not been colour corrected and a histogram colour scale has been used. All-sky maps of thermal dust temperature ({\it{bottom left}}) and spectral index ({\it{bottom right}}) produced by {\texttt{premise}}. Actual temperature range is 10 -- 30 K, actual spectral index range is 1.0 to 2.2.}
\caption{All-sky maps of thermal dust temperature ({\it{top}}), spectral index ({\it{middle}}) and optical depth at 353\,GHz  ({\it{bottom}}) produced by {\texttt{premise}}. The colour scale uses histogram equalisation.}
\label{fig:dustmaps}
\end{figure*}

\begin{figure}
\centering
%\subfloat{\includegraphics[width=0.85\linewidth]{DustOverLat}}\,
\subfloat{\includegraphics[width=0.95\linewidth]{ParamsOverLat}}\,
\caption{Mean absolute percentage difference between true and {\texttt{premise}} estimated parameter values as a function of Galactic latitude. Shaded area represent the standard deviation of the mean values.}
\label{fig:lats}
\end{figure}

Similarly to \citet{gnilc}, the success of {\texttt{premise}} is proportional to the thermal dust signal-to-noise ratio; this is highlighted in Fig.~\ref{fig:lats} which shows the mean absolute percentage difference between true and {\texttt{premise}} estimated values across latitudes. The peak in percentage difference within the Galactic plane (latitude 0$^{\circ}$) is caused by a high fraction of the data being masked due to point sources. This effect can also be seen along the Galactic plane within the temperature and spectral index maps shown in Fig.~\ref{fig:dustmaps}.

\subsection{Discussion} 
 
Obtaining reliable thermal dust estimates from {\it{Planck}} HFI data is complicated by the presence of the CIB which ties the accuracy of any thermal dust estimates to the dust signal-to-noise ratio. {\texttt{premise}} can also be see to suffer from this effect, however, by taking advantage of the sparse nature of thermal dust, {\texttt{premise}} demonstrates an increasing ability to outperform GNILC at parameter estimation as signal-to-noise worsens. This is because instead of continually increasing the level of smoothing to compensate for poor signal to nose, {\texttt{premise}} increases the threshold under which the wavelet coefficients are prevented from contributing to the reconstructed thermal dust estimate. Therefore we can present MBB parameter maps at full resolution. Additionally, by splitting the method into a fast initial estimation and then a refinement step, we introduce a considerable time reduction in the computational time required for {\texttt{premise}} to perform on full sky, $\rm{N_{side}}$ 2048 {\texttt{HEALPix}} maps. 

%\begin{figure}
%\centering
%\subfloat{\includegraphics[width=0.99\linewidth]{premiseFWHM}}
%\caption{All-sky map of FWHM resolution for  {\texttt{premise}} thermal dust maps. Black areas have 5 arcmin resolution whereas white areas have 10 arcmin resolution. }
%\label{fig:fwhm}
%\end{figure}

\section{Conclusions}
We have presented a new method of parametric fitting based around informed, initial estimates of parameter values. Full sky data are divided into patches of various size where each patch area contains only those pixels which share similar thermal dust properties. The MBB model is fit to each patch producing an all-sky estimate of the model parameters and a lower resolution than the raw data. These parameter estimates are then refined using a sparsity-based optimisation method to produce the final, full resolution parameter estimates. In this work we fit for the thermal dust temperature, optical depth at 353\,GHz and spectral index and produce all-sky maps of these three values with median absolute percentage deviations from the true simulation values of 1.7, 3.7 and 3.4 per cent, resepctively.

By comparing {\texttt{premise}} to a GNILC-like method over select regions of the sky we find that {\texttt{premise}} excels in regions of low signal-to-noise, producing more accurate parameter estiamtes than the GNILC-like method in regions outside of the Galactic plane. 

\section*{Acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:
\bibliographystyle{mnras}
\bibliography{refs} % if your bibtex file is called example.bib


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Large-scale CIB offset}\label{sec:apA}

To determine the large-scale CIB offset for each frequency, data from the Leiden/Argentine/Bonn (LAB) $\rm{H_{I}}$ survey \citep{lab} were used. The LAB data are integrated over radial velocity so three maps are available: the low velocity map (-30 to 30 km $\rm{s}^{-1}$), the intermediate velocity map (-100 to -30 km $\rm{s}^{-1}$) and the high velocity map (-500 to -100 km $\rm{s}^{-1}$). 

The total flux (353, 545, 857 and 3000\,GHz) and the LAB low and intermediate velocity all-sky maps were all downgraded to $\rm{N_{side}}$ 128 and smoothed to $1^{\circ}$. Two masks were then made which selected pixels for use depending on their column densities ($N_{\rm{H_{I}}}$) within the low and intermediate velocity maps :

\begin{tabular}{ l | c | c  }
\hline
  Mask & Low $N_{\rm{H_{I}}}$ ($\rm{cm}^{-2}$) & Intermediate $N_{\rm{H_{I}}}$ ($\rm{cm}^{-2}$)  \\
  \hline
  1 &  $< 2 \times 10^{20}$  & $ < 0.1 \times 10^{20}$ \\
  2 &  $ < 3 \times 10^{20}$  & -- \\
  \hline
\end{tabular}

Linear regression was used to determine the large-scale CIB offset at each frequency using the following masked data: 

\begin{tabular}{ l | c | c  }
\hline
 x axis data & y axis data & Mask applied \\
  \hline
  3000\,GHz & low + intermediate LAB & 2 \\ 
  857\,GHz &  low + intermediate LAB & 1 \\
  545\,GHz & 857\,GHz - $\rm{offset}_{857}$ & 1 \\
  353\,GHz & 857\,GHz - $\rm{offset}_{857}$  & 1 \\
  \hline
\end{tabular}

Fig.~\ref{fig:ciboffsetLinear} shows the linear regression between the 545\,GHz and the 857\,GHz total flux binned, mask 1 pixels. The 857\,GHz offset has been removed from the 857\,GHz data for the linear regression. 

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{ciboffsetEx}
\caption{An example of the linear regression used to determine the large scale CIB offset for each frequency.}
\label{fig:ciboffsetLinear}
\end{figure}

\section{GNILC methodology}\label{sec:apB}

The GNILC algorithm analysed in this work is based on that detailed in \citet{gnilc}. Our implementation is described here in full so that any differences may be highlighted. Instead of working on the sphere and using the needlet transform, the total flux maps were divided into twelve 2048X2048 pixel faces and the wavelet transform was used on the 2D faces. The optimum number of wavelet scales was empirically found to be six, instead of the ten scales used by \citet{gnilc}. This difference can be explained by the fact that \citet{gnilc} work on all 12X2048X2048 pixels at once and the number of wavelet scales required is proportional to the log of the number of data samples. 

The following steps were completed for each wavelet scale ($j$):
\begin{enumerate}[label*=\arabic*.]
 \item The total flux maps were smoothed within the wavelet domain via convolution with a Gaussian of FWHM $2^{j} \times \frac{16\, \rm{pixels}}{2 \sqrt{2 \log{2}}}$. This width may be different from that used by \citet{gnilc}.  
 \item The nuisance terms were collected as N = CIB + CMB + instrumental noise. The $\rm{N_{obs}}$ by $\rm{N_{obs}}$ nuisance covariance matrix was calculated as:
\begin{equation}
{\bf{R}}_{{\rm{nus}}} = \frac{1}{\rm{npix}} \left({\bf{N \times N^{T} }} \right),  
\end{equation}     
 \item The covariance matrices of the smoothed total flux maps were calculated as $X_{a} \times X_{b}^{T} $, $X_{a/b}$ being the total flux within the wavelet domain at frequency $a/b$.
\item The $\rm{N_{obs}}$ by $\rm{N_{obs}}$ total flux covariance matrices for each smoothed pixel were whitened: ${\bf{R}}_{{\rm{nus}}}^{-1/2} {\bf{R}}_{{\rm{tot}}} {\bf{R}}_{{\rm{nus}}}^{-1/2}$.
\item The eigenvectors of each whitened covariance matrix were ordered and the Akaike Information Criterion was used to select eigenvalues which deviated significantly from unity. Those eigenvectors (${\bf{U}}_{s}$) gave the mixing matrix (${\bf{F}} = {\bf{R}}_{{\rm{nus}}}^{1/2} {\bf{U}}_{s} $) used to obtain the leat-squares optimisation of thermal dust emission ${\bf{F}} \left( {\bf{F}}^{T} {\bf{R}}_{{\rm{tot}}}^{-1} {\bf{F}} \right) {\bf{F}}^{T} {\bf{R}}_{{\rm{tot}}}^{-1} {\bf{X}}$ 
\item If no eigenvectors were identified as significant then the total flux was believed to be dominated by the nuisance terms and so the signal contribution at that pixel and wavelet scale was masked out (set to zero). 
\end{enumerate}
The thermal dust contributions at each wavelet scale were recomposed to form the GNILC estimate of thermal dust emission for each frequency within pixel space. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex